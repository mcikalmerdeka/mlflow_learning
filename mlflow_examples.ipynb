{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# MLflow Tutorial: Hands-on Examples\n",
        "\n",
        "This notebook provides practical examples of MLflow usage for data science MLOps.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Make sure you have installed the dependencies using UV:\n",
        "```bash\n",
        "uv add mlflow scikit-learn pandas numpy matplotlib seaborn\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Setup and Basic Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLflow tracking URI: file:./mlruns\n",
            "MLflow version: 3.1.4\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import mlflow.models\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris, load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure MLflow\n",
        "tracking_uri = \"file:./mlruns\"\n",
        "mlflow.set_tracking_uri(tracking_uri)\n",
        "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "print(f\"MLflow version: {mlflow.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iris dataset: (150, 4), classes: 3\n",
            "Wine dataset: (178, 13), classes: 3\n"
          ]
        }
      ],
      "source": [
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X_iris, y_iris = iris.data, iris.target\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
        "    X_iris, y_iris, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Load Wine dataset for additional examples\n",
        "wine = load_wine()\n",
        "X_wine, y_wine = wine.data, wine.target\n",
        "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(\n",
        "    X_wine, y_wine, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Iris dataset: {X_iris.shape}, classes: {len(np.unique(y_iris))}\")\n",
        "print(f\"Wine dataset: {X_wine.shape}, classes: {len(np.unique(y_wine))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Basic MLflow Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Created new experiment: iris-classification-tutorial (ID: 623039503078250544)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='file:///e:/mlflow_testing/mlruns/623039503078250544', creation_time=1753396716576, experiment_id='623039503078250544', last_update_time=1753396716576, lifecycle_stage='active', name='iris-classification-tutorial', tags={}>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create experiment\n",
        "experiment_name = \"iris-classification-tutorial\"\n",
        "try:\n",
        "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "    if experiment is None:\n",
        "        experiment_id = mlflow.create_experiment(experiment_name)\n",
        "        print(f\"✅ Created new experiment: {experiment_name} (ID: {experiment_id})\")\n",
        "    else:\n",
        "        experiment_id = experiment.experiment_id\n",
        "        print(f\"✅ Using existing experiment: {experiment_name} (ID: {experiment_id})\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error creating experiment: {e}\")\n",
        "    \n",
        "    # Fallback: create with different approach\n",
        "    experiment_id = mlflow.create_experiment(experiment_name)\n",
        "\n",
        "# Set active experiment\n",
        "mlflow.set_experiment(experiment_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic tracking example - Single model run\n",
        "with mlflow.start_run(run_name=\"baseline-random-forest\"):\n",
        "    \n",
        "    # Single parameter values (not lists!)\n",
        "    n_estimators = 100\n",
        "    max_depth = 10\n",
        "    min_samples_split = 2\n",
        "    \n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
        "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
        "    mlflow.log_param(\"max_depth\", max_depth)\n",
        "    mlflow.log_param(\"min_samples_split\", min_samples_split)\n",
        "    mlflow.log_param(\"dataset\", \"iris\")\n",
        "    mlflow.log_param(\"dataset_size\", len(X_train_iris))\n",
        "    \n",
        "    # Train model\n",
        "    start_time = time.perf_counter()\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train_iris, y_train_iris)\n",
        "    training_time = time.perf_counter() - start_time\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred_train = model.predict(X_train_iris)\n",
        "    y_pred_test = model.predict(X_test_iris)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    train_accuracy = accuracy_score(y_train_iris, y_pred_train)\n",
        "    test_accuracy = accuracy_score(y_test_iris, y_pred_test)\n",
        "    precision = precision_score(y_test_iris, y_pred_test, average='weighted')\n",
        "    recall = recall_score(y_test_iris, y_pred_test, average='weighted')\n",
        "    f1 = f1_score(y_test_iris, y_pred_test, average='weighted')\n",
        "    \n",
        "    # Log metrics\n",
        "    mlflow.log_metrics({\n",
        "        \"train_accuracy\": train_accuracy,\n",
        "        \"test_accuracy\": test_accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1,\n",
        "        \"training_time\": training_time\n",
        "    })\n",
        "    \n",
        "    # Log model with input example and signature (fixes warnings)\n",
        "    input_example = X_train_iris[:5]\n",
        "    mlflow.sklearn.log_model(\n",
        "        model, \n",
        "        \"model\",\n",
        "        input_example=input_example,\n",
        "        signature=mlflow.models.infer_signature(X_train_iris, y_pred_train)\n",
        "    )\n",
        "    \n",
        "    # Save and log confusion matrix\n",
        "    cm = confusion_matrix(y_test_iris, y_pred_test)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix - Random Forest')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.savefig('confusion_matrix_rf.png', dpi=150, bbox_inches='tight')\n",
        "    mlflow.log_artifact('confusion_matrix_rf.png')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Training time: {training_time:.2f} seconds\")\n",
        "    print(f\"Run ID: {mlflow.active_run().info.run_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1. Option 1: Manual Nested Runs (Educational)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HYPERPARAMETER TUNING WITH NESTED RUNS - BEST PRACTICE\n",
        "import itertools\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [5, 10],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "best_model = None\n",
        "\n",
        "# Parent run for the entire tuning experiment\n",
        "with mlflow.start_run(run_name=\"hyperparameter-tuning-experiment\") as parent_run:\n",
        "    \n",
        "    # Log experiment metadata\n",
        "    mlflow.log_param(\"tuning_strategy\", \"grid_search\")\n",
        "    mlflow.log_param(\"param_space\", str(param_grid))\n",
        "    mlflow.log_param(\"total_combinations\", len(list(itertools.product(*param_grid.values()))))\n",
        "    \n",
        "    run_count = 0\n",
        "    \n",
        "    # Grid search with nested runs\n",
        "    for n_est in param_grid['n_estimators']:\n",
        "        for depth in param_grid['max_depth']:\n",
        "            for min_split in param_grid['min_samples_split']:\n",
        "                \n",
        "                run_count += 1\n",
        "                \n",
        "                # Child run for each parameter combination\n",
        "                with mlflow.start_run(run_name=f\"run_{run_count:02d}\", nested=True) as child_run:\n",
        "                    \n",
        "                    # Current parameter combination\n",
        "                    current_params = {\n",
        "                        \"n_estimators\": n_est,\n",
        "                        \"max_depth\": depth,\n",
        "                        \"min_samples_split\": min_split\n",
        "                    }\n",
        "                    \n",
        "                    # Log parameters\n",
        "                    mlflow.log_params({\n",
        "                        **current_params,\n",
        "                        \"model_type\": \"RandomForest\",\n",
        "                        \"dataset\": \"iris\",\n",
        "                        \"random_state\": 42\n",
        "                    })\n",
        "                    \n",
        "                    # Train model\n",
        "                    start_time = time.perf_counter()\n",
        "                    model = RandomForestClassifier(\n",
        "                        n_estimators=n_est,\n",
        "                        max_depth=depth,\n",
        "                        min_samples_split=min_split,\n",
        "                        random_state=42\n",
        "                    )\n",
        "                    model.fit(X_train_iris, y_train_iris)\n",
        "                    training_time = time.perf_counter() - start_time\n",
        "                    \n",
        "                    # Make predictions\n",
        "                    y_pred_train = model.predict(X_train_iris)\n",
        "                    y_pred_test = model.predict(X_test_iris)\n",
        "                    \n",
        "                    # Calculate metrics\n",
        "                    train_accuracy = accuracy_score(y_train_iris, y_pred_train)\n",
        "                    test_accuracy = accuracy_score(y_test_iris, y_pred_test)\n",
        "                    precision = precision_score(y_test_iris, y_pred_test, average='weighted')\n",
        "                    recall = recall_score(y_test_iris, y_pred_test, average='weighted')\n",
        "                    f1 = f1_score(y_test_iris, y_pred_test, average='weighted')\n",
        "                    \n",
        "                    # Log metrics\n",
        "                    mlflow.log_metrics({\n",
        "                        \"train_accuracy\": train_accuracy,\n",
        "                        \"test_accuracy\": test_accuracy,\n",
        "                        \"precision\": precision,\n",
        "                        \"recall\": recall,\n",
        "                        \"f1_score\": f1,\n",
        "                        \"training_time\": training_time\n",
        "                    })\n",
        "                    \n",
        "                    # Log model with signature (always include for best practices)\n",
        "                    input_example = X_train_iris[:3]\n",
        "                    mlflow.sklearn.log_model(\n",
        "                        model, \n",
        "                        \"model\",\n",
        "                        input_example=input_example,\n",
        "                        signature=mlflow.models.infer_signature(X_train_iris, y_pred_test)\n",
        "                    )\n",
        "                    \n",
        "                    # Track best model\n",
        "                    if test_accuracy > best_accuracy:\n",
        "                        best_accuracy = test_accuracy\n",
        "                        best_params = current_params.copy()\n",
        "                        best_model = model\n",
        "                    \n",
        "                    print(f\"Run {run_count:2d} | n_est={n_est:3d}, depth={depth:2d}, min_split={min_split:2d} | Accuracy: {test_accuracy:.4f}\")\n",
        "    \n",
        "    # Log best results to parent run\n",
        "    mlflow.log_params({f\"best_{k}\": v for k, v in best_params.items()})\n",
        "    mlflow.log_metric(\"best_test_accuracy\", best_accuracy)\n",
        "    mlflow.log_metric(\"total_runs\", run_count)\n",
        "    \n",
        "    # Register best model\n",
        "    if best_model is not None:\n",
        "        input_example = X_train_iris[:5]\n",
        "        mlflow.sklearn.log_model(\n",
        "            best_model,\n",
        "            \"best_model\", \n",
        "            input_example=input_example,\n",
        "            signature=mlflow.models.infer_signature(X_train_iris, best_model.predict(X_test_iris)),\n",
        "            registered_model_name=\"iris_best_rf_tuned\"\n",
        "        )\n",
        "    \n",
        "    print(f\"\\n🎯 TUNING COMPLETE!\")\n",
        "    print(f\"📊 Tested {run_count} parameter combinations\")\n",
        "    print(f\"🏆 Best accuracy: {best_accuracy:.4f}\")\n",
        "    print(f\"⚙️  Best parameters: {best_params}\")\n",
        "    print(f\"🔄 Parent run ID: {parent_run.info.run_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 3.5. Option 2: Using GridSearchCV with MLflow Autologging (Recommended for Hyperparameter Tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/07/25 06:01:39 WARNING mlflow.utils.autologging_utils: MLflow sklearn autologging is known to be compatible with 1.3.0 <= scikit-learn <= 1.7.0, but the installed version is 1.7.1. If you encounter errors during autologging, try upgrading / downgrading scikit-learn to a compatible version, or try upgrading MLflow.\n",
            "2025/07/25 06:01:42 INFO mlflow.tracking.fluent: Experiment with name 'iris-gridsearch-autolog' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Running GridSearchCV with 3-fold CV...\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/07/25 06:01:52 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
            "2025/07/25 06:01:56 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
            "2025/07/25 06:01:56 INFO mlflow.sklearn.utils: Logging the 5 best runs, 3 runs will be omitted.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🏆 GRIDSEARCH COMPLETE!\n",
            "📊 Best CV score: 0.9583\n",
            "🎯 Holdout test accuracy: 1.0000\n",
            "⚙️  Best parameters: {'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "🤖 All runs auto-logged to MLflow!\n"
          ]
        }
      ],
      "source": [
        "# OPTION 2: GridSearchCV with MLflow Autologging - EVEN EASIER!\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Enable autologging for sklearn (captures GridSearchCV automatically)\n",
        "mlflow.sklearn.autolog()\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid_cv = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [5, 10],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "# Create experiment for GridSearchCV\n",
        "mlflow.set_experiment(\"iris-gridsearch-autolog\")\n",
        "\n",
        "with mlflow.start_run(run_name=\"gridsearch-with-autolog\") as run:\n",
        "    \n",
        "    # Create base model\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "    \n",
        "    # Create GridSearchCV\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=rf,\n",
        "        param_grid=param_grid_cv,\n",
        "        cv=3,  # 3-fold cross-validation\n",
        "        scoring='accuracy', # set scoring to accuracy\n",
        "        n_jobs=-1,  # Use all cores\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    print(\"🔍 Running GridSearchCV with 3-fold CV...\")\n",
        "    \n",
        "    # Fit - this will automatically log everything to MLflow!\n",
        "    grid_search.fit(X_train_iris, y_train_iris)\n",
        "    \n",
        "    # Get best results\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "    best_score = grid_search.best_score_\n",
        "    \n",
        "    # Test on holdout set\n",
        "    test_accuracy = best_model.score(X_test_iris, y_test_iris)\n",
        "    \n",
        "    # Log additional custom metrics\n",
        "    mlflow.log_metric(\"holdout_test_accuracy\", test_accuracy)\n",
        "    mlflow.log_param(\"cv_folds\", 5)\n",
        "    mlflow.log_param(\"total_combinations_tested\", len(grid_search.cv_results_['mean_test_score']))\n",
        "    \n",
        "    print(f\"\\n🏆 GRIDSEARCH COMPLETE!\")\n",
        "    print(f\"📊 Best CV score: {best_score:.4f}\")\n",
        "    print(f\"🎯 Holdout test accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"⚙️  Best parameters: {best_params}\")\n",
        "    print(f\"🤖 All runs auto-logged to MLflow!\")\n",
        "\n",
        "# Disable autologging\n",
        "mlflow.sklearn.autolog(disable=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 3.6. Option 3: RandomizedSearchCV for Large Parameter Spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTION 3: RandomizedSearchCV for large parameter spaces\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "# Enable autologging again\n",
        "mlflow.sklearn.autolog()\n",
        "\n",
        "# Define LARGE parameter space with distributions\n",
        "param_distributions = {\n",
        "    'n_estimators': randint(50, 300),  # Random integers between 50-300\n",
        "    'max_depth': randint(3, 20),       # Random integers between 3-20\n",
        "    'min_samples_split': randint(2, 20),\n",
        "    'min_samples_leaf': randint(1, 10),\n",
        "    'max_features': uniform(0.1, 0.8)  # Random float between 0.1-0.9\n",
        "}\n",
        "\n",
        "# Set experiment\n",
        "mlflow.set_experiment(\"iris-randomized-search\")\n",
        "\n",
        "with mlflow.start_run(run_name=\"randomized-search-efficient\") as run:\n",
        "    \n",
        "    # Create base model\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "    \n",
        "    # Create RandomizedSearchCV - only test 20 random combinations\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=rf,\n",
        "        param_distributions=param_distributions,\n",
        "        n_iter=20,  # Only test 20 random combinations (vs 100+ in full grid)\n",
        "        cv=3,       # 3-fold CV for speed\n",
        "        scoring='accuracy',\n",
        "        n_jobs=-1,\n",
        "        verbose=1,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    print(\"🎲 Running RandomizedSearchCV (20 random combinations)...\")\n",
        "    \n",
        "    # Fit - auto-logged to MLflow\n",
        "    random_search.fit(X_train_iris, y_train_iris)\n",
        "    \n",
        "    # Get results\n",
        "    best_model = random_search.best_estimator_\n",
        "    best_params = random_search.best_params_\n",
        "    best_score = random_search.best_score_\n",
        "    test_accuracy = best_model.score(X_test_iris, y_test_iris)\n",
        "    \n",
        "    # Log additional metrics\n",
        "    mlflow.log_metric(\"holdout_test_accuracy\", test_accuracy)\n",
        "    mlflow.log_param(\"search_type\", \"randomized\")\n",
        "    mlflow.log_param(\"n_iter\", 20)\n",
        "    \n",
        "    print(f\"\\n🎯 RANDOMIZED SEARCH COMPLETE!\")\n",
        "    print(f\"📊 Best CV score: {best_score:.4f}\")\n",
        "    print(f\"🎯 Holdout test accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"⚙️  Best parameters: {best_params}\")\n",
        "    print(f\"⚡ Much faster than full grid search!\")\n",
        "\n",
        "# Disable autologging\n",
        "mlflow.sklearn.autolog(disable=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 🎯 Hyperparameter Tuning Summary: Choose Your Approach\n",
        "\n",
        "| **Method** | **When to Use** | **Pros** | **Cons** |\n",
        "|------------|-----------------|----------|----------|\n",
        "| **Manual Nested Runs** | Learning MLflow, custom logic needed | Full control, educational | More code, manual loops |\n",
        "| **GridSearchCV + Autolog** | Small parameter spaces, want all combinations | Easy, automatic logging | Can be slow for large grids |\n",
        "| **RandomizedSearchCV + Autolog** | Large parameter spaces, time constraints | Fast, good coverage, automatic | May miss optimal combination |\n",
        "\n",
        "### 🏆 **Best Practices Recommendations:**\n",
        "\n",
        "1. **Start Simple**: Use GridSearchCV with autologging for most cases\n",
        "2. **Go Manual**: Use nested runs when you need custom experiment logic\n",
        "3. **Scale Up**: Use RandomizedSearchCV for large parameter spaces (>100 combinations)\n",
        "4. **Always Log**: Include `input_example` and `signature` in model logging\n",
        "5. **Track Best**: Use parent runs to summarize tuning experiments\n",
        "6. **Name Wisely**: Use descriptive run names and experiment names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Compare Multiple Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different models\n",
        "models_config = [\n",
        "    {\n",
        "        \"name\": \"logistic-regression\",\n",
        "        \"model\": LogisticRegression(random_state=42, max_iter=1000),\n",
        "        \"params\": {\"solver\": \"lbfgs\", \"max_iter\": 1000}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"random-forest-small\",\n",
        "        \"model\": RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42),\n",
        "        \"params\": {\"n_estimators\": 50, \"max_depth\": 5}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"random-forest-large\", \n",
        "        \"model\": RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42),\n",
        "        \"params\": {\"n_estimators\": 200, \"max_depth\": 15}\n",
        "    }\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "with mlflow.start_run(run_name=\"model-comparison\"):\n",
        "    for config in models_config:\n",
        "        with mlflow.start_run(run_name=config[\"name\"], nested=True):\n",
        "            # Log parameters\n",
        "            mlflow.log_param(\"model_type\", config[\"name\"])\n",
        "            mlflow.log_params(config[\"params\"])\n",
        "            \n",
        "            # Train model\n",
        "            start_time = time.time()\n",
        "            model = config[\"model\"]\n",
        "            model.fit(X_train_iris, y_train_iris)\n",
        "            training_time = time.time() - start_time\n",
        "            \n",
        "            # Evaluate\n",
        "            test_accuracy = accuracy_score(y_test_iris, model.predict(X_test_iris))\n",
        "            \n",
        "            # Log metrics\n",
        "            mlflow.log_metrics({\n",
        "                \"test_accuracy\": test_accuracy,\n",
        "                \"training_time\": training_time\n",
        "            })\n",
        "            \n",
        "            # Log model with input example and signature\n",
        "            input_example = X_train_iris[:3]\n",
        "            mlflow.sklearn.log_model(\n",
        "                model, \n",
        "                \"model\",\n",
        "                input_example=input_example,\n",
        "                signature=mlflow.models.infer_signature(X_train_iris, model.predict(X_test_iris))\n",
        "            )\n",
        "            \n",
        "            results.append({\n",
        "                \"model\": config[\"name\"],\n",
        "                \"accuracy\": test_accuracy,\n",
        "                \"training_time\": training_time\n",
        "            })\n",
        "            \n",
        "            print(f\"{config['name']}: {test_accuracy:.4f} accuracy, {training_time:.2f}s\")\n",
        "\n",
        "# Display results\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nModel Comparison Results:\")\n",
        "print(results_df.sort_values('accuracy', ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Hyperparameter Tuning with MLflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning with nested runs\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15, None]\n",
        "}\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "with mlflow.start_run(run_name=\"hyperparameter-tuning\"):\n",
        "    mlflow.log_param(\"tuning_strategy\", \"grid_search\")\n",
        "    mlflow.log_param(\"param_space\", str(param_grid))\n",
        "    \n",
        "    for n_est in param_grid['n_estimators']:\n",
        "        for depth in param_grid['max_depth']:\n",
        "            with mlflow.start_run(nested=True):\n",
        "                # Log parameters\n",
        "                params = {\n",
        "                    \"n_estimators\": n_est,\n",
        "                    \"max_depth\": depth if depth is not None else \"None\"\n",
        "                }\n",
        "                mlflow.log_params(params)\n",
        "                \n",
        "                # Train model\n",
        "                model = RandomForestClassifier(\n",
        "                    n_estimators=n_est, \n",
        "                    max_depth=depth, \n",
        "                    random_state=42\n",
        "                )\n",
        "                model.fit(X_train_iris, y_train_iris)\n",
        "                \n",
        "                # Evaluate\n",
        "                accuracy = accuracy_score(y_test_iris, model.predict(X_test_iris))\n",
        "                mlflow.log_metric(\"test_accuracy\", accuracy)\n",
        "                \n",
        "                # Track best model\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    best_params = params.copy()\n",
        "                    # Log best model with signature\n",
        "                    input_example = X_train_iris[:3]\n",
        "                    mlflow.sklearn.log_model(\n",
        "                        model, \n",
        "                        \"model\",\n",
        "                        input_example=input_example,\n",
        "                        signature=mlflow.models.infer_signature(X_train_iris, model.predict(X_test_iris))\n",
        "                    )\n",
        "                \n",
        "                print(f\"n_est={n_est}, depth={depth}: {accuracy:.4f}\")\n",
        "    \n",
        "    # Log best results to parent run\n",
        "    mlflow.log_params({f\"best_{k}\": v for k, v in best_params.items()})\n",
        "    mlflow.log_metric(\"best_accuracy\", best_accuracy)\n",
        "\n",
        "print(f\"\\nBest accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Model Registration and Versioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final model and register it\n",
        "with mlflow.start_run(run_name=\"production-model\"):\n",
        "    # Use best parameters from tuning\n",
        "    final_model = RandomForestClassifier(\n",
        "        n_estimators=100, \n",
        "        max_depth=10, \n",
        "        random_state=42\n",
        "    )\n",
        "    final_model.fit(X_train_iris, y_train_iris)\n",
        "    \n",
        "    # Evaluate\n",
        "    test_accuracy = accuracy_score(y_test_iris, final_model.predict(X_test_iris))\n",
        "    \n",
        "    # Log everything\n",
        "    mlflow.log_params({\n",
        "        \"n_estimators\": 100,\n",
        "        \"max_depth\": 10,\n",
        "        \"model_purpose\": \"production\"\n",
        "    })\n",
        "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
        "    \n",
        "    # Register model with signature and input example\n",
        "    input_example = X_train_iris[:5]\n",
        "    model_info = mlflow.sklearn.log_model(\n",
        "        final_model, \n",
        "        \"model\",\n",
        "        input_example=input_example,\n",
        "        signature=mlflow.models.infer_signature(X_train_iris, final_model.predict(X_test_iris)),\n",
        "        registered_model_name=\"iris_classifier\"\n",
        "    )\n",
        "    \n",
        "    print(f\"Model registered with accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Model URI: {model_info.model_uri}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Model Registry Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "client = MlflowClient()\n",
        "\n",
        "# List registered models\n",
        "registered_models = client.search_registered_models()\n",
        "print(\"Registered models:\")\n",
        "for model in registered_models:\n",
        "    print(f\"- {model.name}\")\n",
        "    for version in model.latest_versions:\n",
        "        print(f\"  Version {version.version}: {version.current_stage}\")\n",
        "\n",
        "# Get model details\n",
        "if registered_models:\n",
        "    model_name = \"iris_classifier\"\n",
        "    model_version = client.get_latest_versions(model_name)[0]\n",
        "    print(f\"\\nLatest version of {model_name}: {model_version.version}\")\n",
        "    \n",
        "    # Transition to staging\n",
        "    client.transition_model_version_stage(\n",
        "        name=model_name,\n",
        "        version=model_version.version,\n",
        "        stage=\"Staging\"\n",
        "    )\n",
        "    print(f\"Model {model_name} v{model_version.version} moved to Staging\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Loading and Using Registered Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model from registry\n",
        "try:\n",
        "    # Load latest staging model\n",
        "    staging_model = mlflow.pyfunc.load_model(\"models:/iris_classifier/Staging\")\n",
        "    print(\"Loaded model from Staging\")\n",
        "    \n",
        "    # Make predictions\n",
        "    sample_data = X_test_iris[:5]\n",
        "    predictions = staging_model.predict(sample_data)\n",
        "    \n",
        "    print(\"\\nSample predictions:\")\n",
        "    for i, (sample, pred, actual) in enumerate(zip(sample_data, predictions, y_test_iris[:5])):\n",
        "        print(f\"Sample {i+1}: Predicted={pred}, Actual={actual}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"This might happen if no model is in Staging yet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. Advanced Features: Autologging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable autologging\n",
        "mlflow.sklearn.autolog()\n",
        "\n",
        "# Create new experiment for autologging\n",
        "mlflow.set_experiment(\"autologging-demo\")\n",
        "\n",
        "with mlflow.start_run(run_name=\"autolog-example\"):\n",
        "    # Train model - everything is automatically logged!\n",
        "    auto_model = RandomForestClassifier(n_estimators=150, max_depth=12, random_state=42)\n",
        "    auto_model.fit(X_train_wine, y_train_wine)\n",
        "    \n",
        "    # Predictions are also logged\n",
        "    y_pred = auto_model.predict(X_test_wine)\n",
        "    accuracy = accuracy_score(y_test_wine, y_pred)\n",
        "    \n",
        "    print(f\"Autologged model accuracy on wine dataset: {accuracy:.4f}\")\n",
        "    print(\"Check MLflow UI to see all automatically logged parameters and metrics!\")\n",
        "\n",
        "# Disable autologging\n",
        "mlflow.sklearn.autolog(disable=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 10. Custom Artifacts and Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create experiment for custom artifacts\n",
        "mlflow.set_experiment(\"custom-artifacts-demo\")\n",
        "\n",
        "with mlflow.start_run(run_name=\"custom-artifacts\"):\n",
        "    # Train model with preprocessing\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_wine)\n",
        "    X_test_scaled = scaler.transform(X_test_wine)\n",
        "    \n",
        "    model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "    model.fit(X_train_scaled, y_train_wine)\n",
        "    \n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test_wine, y_pred)\n",
        "    \n",
        "    # Log basic metrics\n",
        "    mlflow.log_metric(\"accuracy\", accuracy)\n",
        "    mlflow.log_param(\"preprocessing\", \"StandardScaler\")\n",
        "    \n",
        "    # Save and log preprocessing artifacts\n",
        "    joblib.dump(scaler, \"scaler.pkl\")\n",
        "    mlflow.log_artifact(\"scaler.pkl\", \"preprocessing\")\n",
        "    \n",
        "    # Create feature importance plot\n",
        "    feature_names = wine.feature_names\n",
        "    if hasattr(model, 'coef_'):\n",
        "        importance = np.abs(model.coef_[0])\n",
        "        indices = np.argsort(importance)[::-1][:10]\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.title('Top 10 Feature Importance (Logistic Regression)')\n",
        "        plt.bar(range(10), importance[indices])\n",
        "        plt.xticks(range(10), [feature_names[i] for i in indices], rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
        "        mlflow.log_artifact('feature_importance.png', \"plots\")\n",
        "        plt.show()\n",
        "    \n",
        "    # Create classification report\n",
        "    from sklearn.metrics import classification_report\n",
        "    report = classification_report(y_test_wine, y_pred, target_names=wine.target_names)\n",
        "    \n",
        "    with open(\"classification_report.txt\", \"w\") as f:\n",
        "        f.write(report)\n",
        "    mlflow.log_artifact(\"classification_report.txt\", \"reports\")\n",
        "    \n",
        "    # Log model with signature and input example\n",
        "    input_example = X_train_scaled[:5]\n",
        "    mlflow.sklearn.log_model(\n",
        "        model, \n",
        "        \"model\",\n",
        "        input_example=input_example,\n",
        "        signature=mlflow.models.infer_signature(X_train_scaled, y_pred)\n",
        "    )\n",
        "    \n",
        "    print(f\"Wine classification accuracy: {accuracy:.4f}\")\n",
        "    print(\"Custom artifacts logged: scaler, feature importance plot, classification report\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 11. Experiment Analysis and Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search and compare experiments\n",
        "from mlflow.entities import ViewType\n",
        "\n",
        "# Get all experiments\n",
        "experiments = client.search_experiments()\n",
        "print(\"Available experiments:\")\n",
        "for exp in experiments:\n",
        "    print(f\"- {exp.name} (ID: {exp.experiment_id})\")\n",
        "\n",
        "# Search runs from specific experiment\n",
        "iris_exp = mlflow.get_experiment_by_name(\"iris-classification-tutorial\")\n",
        "if iris_exp:\n",
        "    runs = client.search_runs(\n",
        "        experiment_ids=[iris_exp.experiment_id],\n",
        "        run_view_type=ViewType.ACTIVE_ONLY,\n",
        "        max_results=10\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nRuns in {iris_exp.name}:\")\n",
        "    for run in runs:\n",
        "        metrics = run.data.metrics\n",
        "        params = run.data.params\n",
        "        print(f\"Run: {run.info.run_name}\")\n",
        "        print(f\"  Accuracy: {metrics.get('test_accuracy', 'N/A')}\")\n",
        "        print(f\"  Model: {params.get('model_type', 'N/A')}\")\n",
        "        print(f\"  Status: {run.info.status}\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 12. Cleanup and Best Practices Demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up temporary files\n",
        "import os\n",
        "temp_files = [\n",
        "    'confusion_matrix_rf.png', \n",
        "    'scaler.pkl', \n",
        "    'feature_importance.png',\n",
        "    'classification_report.txt'\n",
        "]\n",
        "\n",
        "for file in temp_files:\n",
        "    if os.path.exists(file):\n",
        "        os.remove(file)\n",
        "        print(f\"Removed {file}\")\n",
        "\n",
        "print(\"\\n=== MLflow Tutorial Complete ===\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Start MLflow UI: uv run mlflow ui\")\n",
        "print(\"2. Open http://localhost:5000 in your browser\")\n",
        "print(\"3. Explore your experiments, runs, and models!\")\n",
        "print(\"4. Try model serving: mlflow models serve -m 'models:/iris_classifier/Staging' -p 5001\")\n",
        "\n",
        "print(\"\\nKey takeaways:\")\n",
        "print(\"- Use experiments to organize related runs\")\n",
        "print(\"- Log parameters, metrics, and artifacts consistently\")\n",
        "print(\"- Use nested runs for hyperparameter tuning\")\n",
        "print(\"- Register important models for production use\")\n",
        "print(\"- Leverage autologging for quick experimentation\")\n",
        "print(\"- Save preprocessing artifacts for reproducibility\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
